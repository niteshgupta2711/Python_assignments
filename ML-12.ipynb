{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXI4XJZEu5C1IUdTrrd4TX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niteshgupta2711/Python_assignments/blob/main/ML-12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is prior probability? Give an example.\n",
        "2. What is posterior probability? Give an example.\n",
        "3. What is likelihood probability? Give an example.\n",
        "\n",
        "4. What is Naïve Bayes classifier? Why is it named so?\n",
        "\n",
        "5. What is optimal Bayes classifier?\n",
        "\n",
        "6. Write any two features of Bayesian learning methods.\n",
        "\n",
        "7. Define the concept of consistent learners.\n",
        "\n",
        "8. Write any two strengths of Bayes classifier.\n",
        "\n",
        "9. Write any two weaknesses of Bayes classifier.\n",
        "\n",
        "10. Explain how Naïve Bayes classifier is used for\n",
        "\n",
        "1. Text classification\n",
        "\n",
        "2. Spam filtering\n",
        "\n",
        "3. Market sentiment analysis"
      ],
      "metadata": {
        "id": "Nze2ZRP_y_bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prior probability shows the likelihood of an outcome in a given dataset. For example, in the mortgage case, P(Y) is the default rate on a home mortgage, which is 2%. P(Y|X) is called the conditional probability, which provides the probability of an outcome given the evidence, that is, when the value of X is known."
      ],
      "metadata": {
        "id": "HIX2lTuH0Zmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the probability that urn A is chosen is 0.5."
      ],
      "metadata": {
        "id": "-8L4Nq3C1eQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Now suppose the same coin is tossed 50 times, and it shows heads only 14 times. You would assume that the likelihood of the unbiased coin is very low. If the coin were fair, it would have shown heads and tails the same number of times"
      ],
      "metadata": {
        "id": "oH8lBMnO1ndV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data; however, the technique is very effective on a large range of complex problems."
      ],
      "metadata": {
        "id": "socMoKb81wuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Bayesian methods aid in understanding other learning algorithms. Training examples have an incremental effect on estimated probabilities of hypothesis correctness. Prior knowledge and observed data combined to determine probabilities of hypotheses. Hypotheses can make probabilistic predictions"
      ],
      "metadata": {
        "id": "Bpouxsik17u0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Bayesian methods aid in understanding other learning algorithms. Training examples have an incremental effect on estimated probabilities of hypothesis correctness. Prior knowledge and observed data combined to determine probabilities of hypotheses. Hypotheses can make probabilistic predictions"
      ],
      "metadata": {
        "id": "H3TwZZDL2MIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Consistent Learners. • A learner L using a hypothesis H and training data D is said to be a consistent learner if it always outputs a hypothesis with zero error on D whenever H contains such a hypothesis. • By definition, a consistent learner must produce a hypothesis in the version space for H given D."
      ],
      "metadata": {
        "id": "l--FSaqE2UWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. It is simple and easy to implement.\n",
        "It doesn't require as much training data.\n",
        "It handles both continuous and discrete data.\n",
        "It is highly scalable with the number of predictors and data points.\n",
        "It is fast and can be used to make real-time predictions."
      ],
      "metadata": {
        "id": "NOi7UEYx2cvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Conditional Independence Assumption does not always hold. ...\n",
        "Zero probability problem : When we encounter words in the test data for a particular class that are not present in the training data, we might end up with zero class probabilities."
      ],
      "metadata": {
        "id": "adW5eRKP2jWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Naive Bayes algorithms are mostly used in sentiment analysis, spam filtering, recommendation systems etc. They are fast and easy to implement but their biggest disadvantage is that the requirement of predictors to be independent."
      ],
      "metadata": {
        "id": "laMinWcc2qhD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JqO5VAya2626"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}