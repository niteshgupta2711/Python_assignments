{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOK6plNPmTU6QYzRMuJTB0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niteshgupta2711/Python_assignments/blob/main/ML-15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Recognize the differences between supervised, semi-supervised, and unsupervised learning.\n",
        "2. Describe in detail any five examples of classification problems.\n",
        "3. Describe each phase of the classification process in detail.\n",
        "\n",
        "4. Go through the SVM model in depth using various scenarios.\n",
        "\n",
        "5. What are some of the benefits and drawbacks of SVM?\n",
        "\n",
        "6. Go over the kNN model in depth.\n",
        "\n",
        "7. Discuss the kNN algorithm&#39;s error rate and validation error.\n",
        "\n",
        "8. For kNN, talk about how to measure the difference between the test and training results.\n",
        "\n",
        "9. Create the kNN algorithm.\n",
        "\n",
        "What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth.\n",
        "\n",
        "11. Describe the different ways to scan a decision tree.\n",
        "\n",
        "12. Describe in depth the decision tree algorithm.\n",
        "\n",
        "13. In a decision tree, what is inductive bias? What would you do to stop overfitting?\n",
        "\n",
        "14.Explain advantages and disadvantages of using a decision tree?\n",
        "\n",
        "15. Describe in depth the problems that are suitable for decision tree learning.\n",
        "\n",
        "16. Describe in depth the random forest model. What distinguishes a random forest?\n",
        "\n",
        "17. In a random forest, talk about OOB error and variable value."
      ],
      "metadata": {
        "id": "JqO5VAya2626"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Within the field of machine learning, there are three main types of tasks: supervised, semi-supervised, and unsupervised. The main difference between these types is the level of availability of ground truth data, which is prior knowledge of what the output of the model should be for a given input."
      ],
      "metadata": {
        "id": "shqL4HrV4Myx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Here are a few interesting examples to illustrate the widespread application of prediction algorithms.\n",
        "1 - Email Spam. ...\n",
        "2 - Handwritten Digit Recognition. ...\n",
        "3 - Image segmentation. ...\n",
        "4 - Speech Recognition. ...\n",
        "5 - DNA Expression Microarray. ...\n",
        "6 - DNA Sequence Classification."
      ],
      "metadata": {
        "id": "l5cXb94a4OOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Classification is a process of categorizing a given set of data into classes, It can be performed on both structured or unstructured data. The process starts with predicting the class of given data points. The classes are often referred to as target, label or categories.\n",
        "\n",
        "The classification predictive modeling is the task of approximating the mapping function from input variables to discrete output variables. The main goal is to identify which class/category the new data will fall into."
      ],
      "metadata": {
        "id": "BYWX2_lI4VPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Support Vector Machine” (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well (look at the below snapshot)."
      ],
      "metadata": {
        "id": "qL7eZL-o4hqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Support vector machine algorithm is not acceptable for large data sets.\n",
        "It does not execute very well when the data set has more sound i.e. target classes are overlapping."
      ],
      "metadata": {
        "id": "LOQoOrIA4u4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a simple case to understand this algorithm. Following is a spread of red circles (RC) and green squares (GS) :\n",
        "\n",
        "scenario1You intend to find out the class of the blue star (BS). BS can either be RC or GS and nothing else. The “K” is KNN algorithm is the nearest neighbor we wish to take the vote from. Let’s say K = 3. Hence, we will now make a circle with BS as the center just as big as to enclose only three datapoints on the plane. Refer to the following diagram for more details:\n",
        "\n",
        "scenario2The three closest points to BS is all RC. Hence, with a good confidence level, we can say that the BS should belong to the class RC. Here, the choice became very obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. Next, we will understand what are the factors to be considered to conclude the best K.\n",
        "\n",
        " \n",
        "\n",
        "How do we choose the factor K?\n",
        "First let us try to understand what exactly does K influence in the algorithm. If we see the last example, given that all the 6 training observation remain constant, with a given K value we can make boundaries of each class. These boundaries will segregate RC from GS. In the same way, let’s try to see the effect of value “K” on the class boundaries. The following are the different boundaries separating the two classes with different values of K.\n",
        "\n",
        "K judgement\n",
        "\n",
        "K judgement2\n",
        "\n",
        "If you watch carefully, you can see that the boundary becomes smoother with increasing value of K. With K increasing to infinity it finally becomes all blue or all red depending on the total majority.  The training error rate and the validation error rate are two parameters we need to access different K-value. Following is the curve for the training error rate with a varying value of K :\n",
        "\n",
        "training errorAs you can see, the error rate at K=1 is always zero for the training sample. This is because the closest point to any training data point is itself.Hence the prediction is always accurate with K=1. If validation error curve would have been similar, our choice of K would have been 1. Following is the validation error curve with varying value of K:\n",
        "\n",
        "training error_1This makes the story more clear. At K=1, we were overfitting the boundaries. Hence, error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. To get the optimal value of K, you can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K. This value of K should be used for all predictions."
      ],
      "metadata": {
        "id": "-GDjbbzr5I9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Hence, error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. To get the optimal value of K, you can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K.MM"
      ],
      "metadata": {
        "id": "OXLwGvag5Znm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Decision Tree Algorithm\n",
        "Decision Tree algorithm belongs to the family of supervised learning algorithms. Unlike other supervised learning algorithms, the decision tree algorithm can be used for solving regression and classification problems too.\n",
        "\n",
        "The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).\n",
        "\n",
        "In Decision Trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with the record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node."
      ],
      "metadata": {
        "id": "Z2sLn_a15h_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Advantages\n",
        "Decision trees generate understandable rules.\n",
        "Decision trees perform classification without requiring much computation.\n",
        "Decision trees are capable of handling both continuous and categorical variables.\n",
        "Decision trees provide a clear indication of which fields are most important for prediction or classification.\n",
        "Disadvantages\n",
        "Decision trees are less appropriate for estimation tasks where the goal is to predict the value of a continuous attribute.\n",
        "Decision trees are prone to errors in classification problems with many class and a relatively small number of training examples.\n",
        "Decision trees can be computationally expensive to train. The process of growing a decision tree is computationally expensive. At each node, each candidate splitting field must be sorted before its best split can be found. In some algorithms, combinations of fields are used and a search must be made for optimal combining weights. Pruning algorithms can also be expensive since many candidate sub-trees must be formed and compared."
      ],
      "metadata": {
        "id": "CNdfPhr55yO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. By default, the decision tree model is allowed to grow to its full depth. Pruning refers to a technique to remove the parts of the decision tree to prevent growing to its full depth. By tuning the hyperparameters of the decision tree model one can prune the trees and prevent them from overfitting.\n",
        "\n",
        "There are two types of pruning Pre-pruning and Post-pruning. Now let's discuss the in-depth understanding and hands-on implementation of each of these pruning techniques.\n",
        "\n",
        "Pre-Pruning:\n",
        "The pre-pruning technique refers to the early stopping of the growth of the decision tree. The pre-pruning technique involves tuning the hyperparameters of the decision tree model prior to the training pipeline. The hyperparameters of the decision tree including max_depth, min_samples_leaf, min_samples_split can be tuned to early stop the growth of the tree and prevent the model from overfitting.\n",
        "\n",
        "\n",
        "(Image by Author), AUC-ROC score vs max depth\n",
        "As observed from the above plot, with an increase in max_depth training AUC-ROC score continuously increases, but the test AUC score remains constants after a value of max depth. The best-fit decision tree is at a max depth value of 5. Increase the max depth value further can cause an overfitting problem.\n",
        "\n",
        "max_depth, min_samples_leaf, min_samples_splitare other hyperparameters of the decision tree algorithm that can be tuned to get a robust model. One can use the sklearn implementation of the GridSearchCV technique to find the best set of hyperparameters for a decision tree model.\n",
        "\n",
        "\n",
        "(Code by Author)\n",
        "Post-Pruning:\n",
        "The Post-pruning technique allows the decision tree model to grow to its full depth, then removes the tree branches to prevent the model from overfitting. Cost complexity pruning (ccp) is one type of post-pruning technique. In case of cost complexity pruning, the ccp_alpha can be tuned to get the best fit model.\n",
        "\n",
        "Scikit-learn package comes with the implementation to compute the ccp_alpha values of the decision tree using function cost_complexity_pruning_path(). With the increase in ccp_apha values, more nodes of the tree are pruned.\n",
        "\n",
        "Steps for cost complexity pruning (post-pruning) are:\n",
        "\n",
        "Train a decision tree classifier to its full depth (default hyperparameters).\n",
        "Compute the ccp_alphas value using function cost_complexity_pruning_path().\n",
        "\n",
        "(Image by Author), ccp_alpha values\n",
        "Train decision tree classifiers with different values of ccp_alphas and compute train and test performance scores.\n",
        "Plot train and test scores for each value of ccp_alphas values.\n",
        "\n",
        "(Image by Author), AUC-ROC score vs ccp_alphas\n",
        "From the above plot, ccp_alpha=0.000179 can be considered as the best parameter as AUC-ROC scores for train and test are 0.94 and 0.92 respectively.\n",
        "\n",
        "\n",
        "(Code by Author), Post-pruning"
      ],
      "metadata": {
        "id": "u4OTZXRY6Sjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Random forest model is a bagging-type ensemble (collection) of decision trees that trains several trees in parallel and uses the majority decision of the trees as the final decision of the random forest model. Individual decision tree model is easy to interpret but the model is nonunique and exhibits high variance."
      ],
      "metadata": {
        "id": "zVGOwJa76hLl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8ljIJLE6tYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}