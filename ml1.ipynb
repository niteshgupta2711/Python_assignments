{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62680897",
   "metadata": {},
   "source": [
    "1. What does one mean by the term &quot;machine learning&quot;?\n",
    "\n",
    "2.Can you think of 4 distinct types of issues where it shines?\n",
    "\n",
    "3.What is a labeled training set, and how does it work?\n",
    "\n",
    "4.What are the two most important tasks that are supervised?\n",
    "\n",
    "5.Can you think of four examples of unsupervised tasks?\n",
    "\n",
    "6.State the machine learning model that would be best to make a robot walk through various\n",
    "unfamiliar terrains?\n",
    "\n",
    "7.Which algorithm will you use to divide your customers into different groups?\n",
    "\n",
    "8.Will you consider the problem of spam detection to be a supervised or unsupervised learning\n",
    "problem?\n",
    "\n",
    "9.What is the concept of an online learning system?\n",
    "\n",
    "10.What is out-of-core learning, and how does it differ from core learning?\n",
    "\n",
    "11.What kind of learning algorithm makes predictions using a similarity measure?\n",
    "\n",
    "12.What&#39;s the difference between a model parameter and a hyperparameter in a learning\n",
    "algorithm?\n",
    "\n",
    "13.What are the criteria that model-based learning algorithms look for? What is the most popular\n",
    "method they use to achieve success? What method do they use to make predictions?\n",
    "\n",
    "14.Can you name four of the most important Machine Learning challenges?\n",
    "\n",
    "15.What happens if the model performs well on the training data but fails to generalize the results\n",
    "to new situations? Can you think of three different options?\n",
    "\n",
    "16.What exactly is a test set, and why would you need one?\n",
    "\n",
    "17.What is a validation set&#39;s purpose?\n",
    "\n",
    "18.What precisely is the train-dev kit, when will you need it, how do you put it to use?\n",
    "\n",
    "19.What could go wrong if you use the test set to tune hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9f6af",
   "metadata": {},
   "source": [
    "1. Ability of a machine to learn from expeience rather than explcitly being code\n",
    "### machine \n",
    "### learning in machine learning is parameters being adjusted while decresing the error by taking the jacobian of loss sfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99047954",
   "metadata": {},
   "source": [
    "2. ## machine learning excels at almost all fields\n",
    " 1. Analytics, business, medical, security , robotics etc...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07888d4",
   "metadata": {},
   "source": [
    "3. Labelled trainig in machine learning is supervised learning\n",
    " + It requires labelled data to train\n",
    " + labels are tagged as dependent varaible , independent variables are trained to map dependent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02cef7",
   "metadata": {},
   "source": [
    "4. Regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d28c8",
   "metadata": {},
   "source": [
    "5. Clustering, Customer Segmentation, pattern matching, similar tensor finding etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf407fed",
   "metadata": {},
   "source": [
    "6. if it is supervised we can use clasification\n",
    " + other best form of model for robotics is reinforcent learning vastly used in robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570acbde",
   "metadata": {},
   "source": [
    "7. Kmeans algorithm or Autoencoders segemntation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e291585",
   "metadata": {},
   "source": [
    "8. spam detectio is supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b6a1f",
   "metadata": {},
   "source": [
    "9. Online learning system is a concept where model adapts to data as soon as it is available \n",
    "  + eg- youtube recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c2b86",
   "metadata": {},
   "source": [
    "10. Ability of a machine to learn from in memory data is core learning\n",
    " + Out-of-core (or “external memory”) learning is a technique used to learn from data that cannot fit in a computer’s main memory (RAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8907b",
   "metadata": {},
   "source": [
    "Euclidean distance\n",
    "Euclidean distance implementation in python\n",
    "Manhattan distance\n",
    "Manhattan distance implementation in python\n",
    "Minkowski distance\n",
    "Synonyms of Minkowski\n",
    "Minkowski distance implementation in python\n",
    "Cosine Similarity\n",
    "Cosine Similarity Implementation In Python\n",
    "Jaccard Similarity\n",
    "Sets & Set Operations\n",
    "Jaccard Similarity implementation in python\n",
    "Implementations of all five similarity measures implementation in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f42ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters are stsatics values while raing like leearning rate , batch_size etc\n",
    "# where as model aprameters are parameters  that chage while traing to get optimized model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd29168",
   "metadata": {},
   "source": [
    "### 13) What do model based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n",
    "The goal for a model-based algorithm is to be able to generalize to new examples. To do this, model based algorithms search for optimal values for the model's parameters, often called `theta`. This searching, or \"learning\", is what machine learning is all about. Model-based system learn by minimizing a cost function that measures how bad the system is at making predicitons on new data, plus a penalty for model complexity if the model is regularized. To make a prediction, a new instance's features are fed into a hypothesis function which uses the minimized `theta` found by repeatedly running the cost function.\n",
    "\n",
    "### 14) Can you name 4 of the main challenges in Machine Learning?\n",
    "* Not gathering enough data, or sampling noise. Sampling noise means we'll have non-representative data as a result of chance.\n",
    "\n",
    "* Using a dataset that is not representative of the cases you want to generalize to. This is called sampling bias. For example, if you want to train an algorithm with \"cat videos\", and all your videos are from YouTube, you're actually training an algorithm to learn about \"YouTube cat videos.\"\n",
    "\n",
    "* Your dataset is full of missing values, outliers, and noise (poor measurments).\n",
    "\n",
    "* The features in your dataset are irrelevant. Garbage in, garbage out. \n",
    "  - Feature selection - choose the most relevant features from your dataset\n",
    "  - Feature extraction - combine features in your dataset to generate a new, more useful feature\n",
    "  \n",
    "* When your model performs well on the training data, but not on test data, you've over fit your model. Models that suffer from overfitting do not generalize well to new examples. Overfitting happens when the model is too complex relative to the amount and noisiness of the data.\n",
    "  - Try simplyfying the model by reducing the number of features in the data or constraining the parameters by reducing the degrees of freedom.\n",
    "  - Gather more training data.\n",
    "  - Reduce noise in the training data by fixing errors and removing outliers.\n",
    "\n",
    "* When your model is too simple to learn the underlying structure of the data you've underfit your model.\n",
    "  - Select a more powerful model with more parameters\n",
    "  - Use feature engineering to feed better features to the model\n",
    "  - Reduce the constraints of the model (increase degrees of freedom, reduce regularization parameter, etc.)\n",
    "\n",
    "\n",
    "### 15) If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name 3 possible solutions?\n",
    "This is a case where the model is overfitting the training data. To couteract overfitting, we can reduce the complexity of the model by removing features or constraining the parameters. We could gather more data. Finally we can reduce noisiness in the data by fixing errors and removing outliers.\n",
    "\n",
    "### 16) What is a test set and why would you want to use it?\n",
    "When we want to know how well our model generalizes to new cases we prefer to use a test set instead of actually deploying the system. To build the test set we split the training data (50-50, 60-40, 80-20 are common splits) into a training set and test set. Our model is training with the training set. Then we use the model to run predictions on the test set. Our error rate on the test set is called the `generalization error` or `out-of-sample error`. This error tells us how well our model performs on examples it has never seen before. \n",
    "\n",
    "If the training error is low, but the generalization error is high, it means we're overfitting our model.\n",
    "\n",
    "### 17) What is the purpose of a validation set?\n",
    "Let's say we have a linear model and we want to perform some hyperparameter tuning to reduce the generalization error. One way to do this 100 different models with 100 different hyperparameter values using the training set and finding the generalization error with the test set. You find the best hyperparameter value gives you 5% generalization error.\n",
    "\n",
    "So you launch the model into production and find you're seeing 15% generalization error. This isn't going as expected. What happened? \n",
    "\n",
    "The problem is that for each iteration of hyperparameter tuning, you measured the generalization error then updated the model using the same test set. In other words, your produced the best generalization error for the test set. The test set no longer represents cases the model hasn't seen before.\n",
    "\n",
    "A common solution to this problem is to have a second holdout set called the validation set. You train multiple models with various hyperparameters using the training set, you select the model and hyperparameters that perform best on the validation set, and when you are happy about your model you run a single final test against the test set to get an estimate of the generalization error.\n",
    "\n",
    "### 18) What can go wrong if you tune hyperparameters using the test set?\n",
    "Your model will not be generalizable to new examples.\n",
    "\n",
    "### 19) What is cross-validation and why would you prefer it to a validation set?\n",
    "Cross-validation helps us compare models without wasting too much training data in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49b421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
