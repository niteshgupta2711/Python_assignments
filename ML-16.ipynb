{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxJ50lri3NHdllWBIQYidV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niteshgupta2711/Python_assignments/blob/main/ML-16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In a linear equation, what is the difference between a dependent variable and an independent\n",
        "variable?\n",
        "2. What is the concept of simple linear regression? Give a specific example.\n",
        "3. In a linear regression, define the slope.\n",
        "\n",
        "4. Determine the graph&#39;s slope, where the lower point on the line is represented as (3, 2) and the\n",
        "higher point is represented as (2, 2).\n",
        "\n",
        "5. In linear regression, what are the conditions for a positive slope?\n",
        "\n",
        "6. In linear regression, what are the conditions for a negative slope?\n",
        "\n",
        "7. What is multiple linear regression and how does it work?\n",
        "\n",
        "8. In multiple linear regression, define the number of squares due to error.\n",
        "\n",
        "9. In multiple linear regression, define the number of squares due to regression.\n",
        "\n",
        "In a regression equation, what is multicollinearity?\n",
        "\n",
        "11. What is heteroskedasticity, and what does it mean?\n",
        "\n",
        "12. Describe the concept of ridge regression.\n",
        "\n",
        "13. Describe the concept of lasso regression.\n",
        "\n",
        "14. What is polynomial regression and how does it work?\n",
        "\n",
        "15. Describe the basis function.\n",
        "\n",
        "16. Describe how logistic regression works."
      ],
      "metadata": {
        "id": "A8ljIJLE6tYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. If x and y are two variables in an algebraic equation and every value of x is linked with any other value of y, then 'y' value is said to be a function of x value known as an independent variable, and 'y' value is known as a dependent variable."
      ],
      "metadata": {
        "id": "DF7K0fRu7HoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. We could use the equation to predict weight if we knew an individual's height. In this example, if an individual was 70 inches tall, we would predict his weight to be: Weight = 80 + 2 x (70) = 220 lbs. In this simple linear regression, we are examining the impact of one independent variable on the outcome"
      ],
      "metadata": {
        "id": "1SAY0H_H7RkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0)."
      ],
      "metadata": {
        "id": "BUYL-5ga7XmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. n summary, if the slope is positive, y increases as x increases, and the function runs \"uphill\" (going left to right). If the slope is negative, y decreases as x increases and the function runs downhill. If the slope is zero, y does not change, thus is constant—a horizontal line."
      ],
      "metadata": {
        "id": "_4sh4Bil7f7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. If the slope is negative, y decreases as x increases and the function runs downhill. If the slope is zero, y does not change, thus is constant—a horizontal line. Vertical lines are problematic in that there is no change in x. Thus our formula is undefined due to division by zero."
      ],
      "metadata": {
        "id": "U-n795oc7n8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Multiple linear regression refers to a statistical technique that uses two or more independent variables to predict the outcome of a dependent variable. The technique enables analysts to determine the variation of the model and the relative contribution of each independent variable in the total variance"
      ],
      "metadata": {
        "id": "7RRtlN3t7uJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Sum of squares (SS) is a statistical tool that is used to identify the dispersion of data as well as how well the data can fit the model in regression analysis. The sum of squares got its name because it is calculated by finding the sum of the squared differences."
      ],
      "metadata": {
        "id": "8UTxhdS771li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal of multiple linear regression is to model the linear relationship between the explanatory (independent) variables and response (dependent) variables. In essence, multiple regression is the extension of ordinary least-squares (OLS) regression because it involves more than one explanatory variable."
      ],
      "metadata": {
        "id": "bGM0-Yc-7_az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. In statistics, a sequence of random variables is homoscedastic if all its random variables have the same finite variance. This is also known as homogeneity of variance. The complementary notion is called heteroscedasticity. The spellings homoskedasticity and heteroskedasticity are also frequently used."
      ],
      "metadata": {
        "id": "CQag3I8v8HtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. / 13. Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization."
      ],
      "metadata": {
        "id": "MYF03Qcn8Xlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x."
      ],
      "metadata": {
        "id": "qyK8WSbM8kRS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Xj_Vu_V8q9h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}