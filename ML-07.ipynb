{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNghI5262rMx1I7u4eOk3GC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niteshgupta2711/Python_assignments/blob/main/ML-07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the definition of a target function? In the sense of a real-life example, express the target\n",
        "function. How is a target function&#39;s fitness assessed?\n",
        "2. What are predictive models, and how do they work? What are descriptive types, and how do you\n",
        "use them? Examples of both types of models should be provided. Distinguish between these two\n",
        "forms of models.\n",
        "3. Describe the method of assessing a classification model&#39;s efficiency in detail. Describe the various\n",
        "measurement parameters.\n",
        "4.\n",
        "i. In the sense of machine learning models, what is underfitting? What is the most common\n",
        "reason for underfitting?\n",
        "ii. What does it mean to overfit? When is it going to happen?\n",
        "iii. In the sense of model fitting, explain the bias-variance trade-off.\n",
        "5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.\n",
        "6. How would you rate an unsupervised learning model&#39;s success? What are the most common\n",
        "success indicators for an unsupervised learning model?\n",
        "7. Is it possible to use a classification model for numerical data or a regression model for categorical\n",
        "data with a classification model? Explain your answer.\n",
        "8. Describe the predictive modeling method for numerical values. What distinguishes it from\n",
        "categorical predictive modeling?\n",
        "9. The following data were collected when using a classification model to predict the malignancy of a\n",
        "group of patients&#39; tumors:\n",
        "i. Accurate estimates – 15 cancerous, 75 benign\n",
        "ii. Wrong predictions – 3 cancerous, 7 benign\n",
        "Determine the model&#39;s error rate, Kappa value, sensitivity, precision, and F-measure.\n",
        "10. Make quick notes on:\n",
        "1. The process of holding out\n",
        "2. Cross-validation by tenfold\n",
        "3. Adjusting the parameters\n",
        "11. Define the following terms:\n",
        "1. Purity vs. Silhouette width\n",
        "2. Boosting vs. Bagging\n",
        "3. The eager learner vs. the lazy learner"
      ],
      "metadata": {
        "id": "qDzuPkEYEbPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A target function, in machine learning, is a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis).\n"
      ],
      "metadata": {
        "id": "GIE-yxcGN7F8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. + Descriptive Analytics, which use data aggregation and data mining to provide insight into the past and answer: “What has happened?”\n",
        "+ Predictive Analytics, which use statistical models and forecasting techniques to understand the future and answer: “What could happen?”\n",
        "+ Prescriptive Analytics, which use optimization and simulation algorithms to advise on possible outcomes and answer: “What should we do?”"
      ],
      "metadata": {
        "id": "NVBRavm8OyNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Confusion Matrix\n",
        "+ Precision\n",
        "+ Recall/ Sensitivity\n",
        "+ Specificity\n",
        "+ F1-Score\n",
        "+ AUC & ROC Curve"
      ],
      "metadata": {
        "id": "E5Fz1dsfSnZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. IN machine learning terms underfititing means model is not performing good on either of training or test Data\n",
        "+ Most common reason for underfitting is data cleaning not done properly, not complete, model not architetcured correctly, feature engineering not "
      ],
      "metadata": {
        "id": "ThYWWPnXS_lR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. yes by smoothening average direction to global minima, decresing learning rat e over epochs"
      ],
      "metadata": {
        "id": "tDm7vQuRUmIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. In general. the evaluation of clustering algorithms is difficult because it’s subjective what success is. There’s no well-defined metric for clustering — “make this easier to understand” is hard to define mathematically.\n",
        "\n",
        "One common way that clustering algorithms are evaluated from a research perspective (“is this a good clustering algorithm in general” and not “is this a good clustering algorithm for this application”) is as follows:\n",
        "\n",
        "Take a labeled dataset (e.g. MNIST).\n",
        "Remove the labels.\n",
        "Apply the clustering algorithm in question.\n",
        "See if the clusters are correlated with the known labels (e.g. the 10 digits). An ideal solution would find exactly 10 clusters, but some overclustering (e.g. 1 and 7 ending up in the same cluster) or underclustering (eg “7” and “7 with a strike-through”) may be acceptable.\n",
        "This isn’t very satisfying, but it’s the best that is commonly used."
      ],
      "metadata": {
        "id": "H1bTW2cjU5CS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. it is possible if correct prediction analysis been done with  preidictor values"
      ],
      "metadata": {
        "id": "gH2xi6pRWp6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Predictive modelling : we predict a continus value with numerical feature, and a discrete value with categorical feature"
      ],
      "metadata": {
        "id": "hPtFeEQ9a31I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 Cross validation by 10 fold usually means divide the dataset in 10 eqauully len ds , fit the data to model on 9/10 of data, validate metrics on 1/10 data"
      ],
      "metadata": {
        "id": "Vvokl60jbD_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. AAdjusting parameters Either hyper parameter tuning , or by gradient descent algorithm"
      ],
      "metadata": {
        "id": "tk9tA_xjcEfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 Boosting is a ensemble technique sequential learning , mostly base model for this is decision There’s\n",
        "bagging : one can fit the data to model parallely, no sequential dependency."
      ],
      "metadata": {
        "id": "rZJug13jcq83"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8jzzFWEfdW5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}